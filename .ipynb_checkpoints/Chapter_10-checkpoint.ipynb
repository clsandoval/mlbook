{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-3eabbfcacf84>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = (iris.target == 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "per_clf =Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "y_pred = per_clf.predict([[2,0.5]])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full,y_train_full),(X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "## images represented as 28x28 array\n",
    "## pixels are integers 0-255\n",
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data and create validation set\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0 \n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "class_names = [\"Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13fb566fe80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sample images\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.concatenate(X_train[0:5], axis = 1), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    (keras.layers.Flatten(input_shape = [28,28])),\n",
    "    (keras.layers.Dense(300, activation = \"relu\")),\n",
    "    (keras.layers.Dense(100, activation = \"relu\")),\n",
    "    (keras.layers.Dense(10, activation = \"softmax\"))\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9925 - accuracy: 0.6943 - val_loss: 0.5015 - val_accuracy: 0.8360\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5015 - accuracy: 0.8259 - val_loss: 0.4581 - val_accuracy: 0.8388\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4478 - accuracy: 0.8452 - val_loss: 0.4615 - val_accuracy: 0.8356\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4187 - accuracy: 0.8515 - val_loss: 0.4173 - val_accuracy: 0.8556\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3962 - accuracy: 0.8627 - val_loss: 0.3909 - val_accuracy: 0.8614\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3799 - accuracy: 0.8665 - val_loss: 0.3687 - val_accuracy: 0.8720\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3692 - accuracy: 0.8696 - val_loss: 0.3796 - val_accuracy: 0.8708\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3591 - accuracy: 0.8734 - val_loss: 0.3568 - val_accuracy: 0.8758\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3413 - accuracy: 0.8785 - val_loss: 0.3506 - val_accuracy: 0.8762\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3352 - accuracy: 0.8803 - val_loss: 0.3572 - val_accuracy: 0.8718\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3251 - accuracy: 0.8838 - val_loss: 0.3387 - val_accuracy: 0.8826\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3156 - accuracy: 0.8882 - val_loss: 0.3396 - val_accuracy: 0.8794\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3155 - accuracy: 0.8868 - val_loss: 0.3283 - val_accuracy: 0.8844\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3090 - accuracy: 0.8882 - val_loss: 0.3361 - val_accuracy: 0.8816\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3002 - accuracy: 0.8928 - val_loss: 0.3397 - val_accuracy: 0.8786\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2967 - accuracy: 0.8923 - val_loss: 0.3160 - val_accuracy: 0.8866\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2892 - accuracy: 0.8954 - val_loss: 0.3202 - val_accuracy: 0.8860\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2797 - accuracy: 0.8996 - val_loss: 0.3158 - val_accuracy: 0.8868\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2785 - accuracy: 0.9001 - val_loss: 0.3518 - val_accuracy: 0.8686\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2737 - accuracy: 0.9015 - val_loss: 0.3261 - val_accuracy: 0.8862\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2634 - accuracy: 0.9060 - val_loss: 0.3094 - val_accuracy: 0.8898\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2633 - accuracy: 0.9039 - val_loss: 0.3060 - val_accuracy: 0.8904\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2572 - accuracy: 0.9074 - val_loss: 0.3100 - val_accuracy: 0.8838\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2525 - accuracy: 0.9086 - val_loss: 0.3099 - val_accuracy: 0.8856\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2451 - accuracy: 0.9118 - val_loss: 0.3134 - val_accuracy: 0.8874\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2438 - accuracy: 0.9120 - val_loss: 0.3124 - val_accuracy: 0.8834\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2450 - accuracy: 0.9111 - val_loss: 0.3056 - val_accuracy: 0.8910\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2382 - accuracy: 0.9149 - val_loss: 0.3021 - val_accuracy: 0.8912\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2297 - accuracy: 0.9162 - val_loss: 0.3126 - val_accuracy: 0.8850\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2322 - accuracy: 0.9170 - val_loss: 0.2939 - val_accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs = 30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 59.1534 - accuracy: 0.8534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[59.153411865234375, 0.8533999919891357]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle Boot', 'Pullover', 'Trouser'], dtype='<U10')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_new), axis = 1)\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13fc3e22880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.concatenate(X_test[:3],axis = 1), cmap  = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.4029 - val_loss: 0.6162\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5146 - val_loss: 0.6524\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4899 - val_loss: 0.6049\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4519 - val_loss: 0.4462\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4085\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4057\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.3961\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4029 - val_loss: 0.3867\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.3833\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.3750\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3697 - val_loss: 0.4067\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.3716\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3801 - val_loss: 0.3694\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.3637\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3749 - val_loss: 0.3624\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3694 - val_loss: 0.5638\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3793 - val_loss: 0.3644\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.3739\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.3621\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3509 - val_loss: 0.3601\n",
      "162/162 [==============================] - 0s 878us/step - loss: 0.3584\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = \"sgd\")\n",
    "history = model.fit(X_train,y_train,epochs = 20, validation_data = (X_val, y_val))\n",
    "mse_test = model.evaluate(X_test,y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Complex Models with Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.8417 - val_loss: 0.9089\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8060 - val_loss: 0.7494\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7442 - val_loss: 0.6859\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6918 - val_loss: 0.6485\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6423 - val_loss: 0.6123\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6056 - val_loss: 0.5840\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5838 - val_loss: 0.5812\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.5385\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5311\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5173 - val_loss: 0.5136\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5116 - val_loss: 0.4969\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4948 - val_loss: 0.4914\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4851 - val_loss: 0.4824\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4935 - val_loss: 0.4709\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.4615\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4741 - val_loss: 0.4675\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4657 - val_loss: 0.4564\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4538\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4537 - val_loss: 0.4492\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4425 - val_loss: 0.4406\n",
      "162/162 [==============================] - 0s 947us/step - loss: 0.4409\n"
     ]
    }
   ],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30,activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30,activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs= [input_],outputs = [output])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_val, y_val))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3.4121 - val_loss: 0.8915\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7788 - val_loss: 0.6721\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6722 - val_loss: 0.6085\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6027 - val_loss: 0.5724\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5680 - val_loss: 0.5443\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5544 - val_loss: 0.5235\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5413 - val_loss: 0.5052\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5181 - val_loss: 0.4916\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5215 - val_loss: 0.4801\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4972 - val_loss: 0.4687\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4877 - val_loss: 0.4611\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4716 - val_loss: 0.4527\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4426\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4856 - val_loss: 0.4400\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4593 - val_loss: 0.4339\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4532 - val_loss: 0.4344\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4339 - val_loss: 0.4294\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4477 - val_loss: 0.4229\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4384 - val_loss: 0.4191\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4185\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name =\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1,name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A,input_B], outputs = [output])\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = keras.optimizers.SGD(lr=1e-3))\n",
    "X_train_A, X_train_B = X_train[:,:5], X_train[:,2:]\n",
    "X_val_A, X_val_B = X_val[:,:5], X_val[:,2:]\n",
    "X_test_A, X_test_B = X_test[:,:5], X_test[:,2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "model.fit((X_train_A,X_train_B),y_train, epochs = 20, validation_data = ((X_val_A,X_val_B),y_val))\n",
    "y_pred=model.predict((X_new_A,X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.5227 - output_loss: 1.3240 - aux_output_loss: 3.3107 - val_loss: 0.6417 - val_output_loss: 0.5669 - val_aux_output_loss: 1.3155\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6014 - output_loss: 0.5352 - aux_output_loss: 1.1973 - val_loss: 0.6105 - val_output_loss: 0.5633 - val_aux_output_loss: 1.0356\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5374 - output_loss: 0.4842 - aux_output_loss: 1.0168 - val_loss: 0.5234 - val_output_loss: 0.4823 - val_aux_output_loss: 0.8933\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4879 - output_loss: 0.4449 - aux_output_loss: 0.8753 - val_loss: 0.4984 - val_output_loss: 0.4641 - val_aux_output_loss: 0.8070\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.0085 - output_loss: 0.9895 - aux_output_loss: 1.1800 - val_loss: 0.5721 - val_output_loss: 0.5280 - val_aux_output_loss: 0.9684\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5051 - output_loss: 0.4598 - aux_output_loss: 0.9134 - val_loss: 0.5967 - val_output_loss: 0.5778 - val_aux_output_loss: 0.7673\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4667 - output_loss: 0.4358 - aux_output_loss: 0.7448 - val_loss: 0.4642 - val_output_loss: 0.4410 - val_aux_output_loss: 0.6733\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4504 - output_loss: 0.4257 - aux_output_loss: 0.6728 - val_loss: 0.4233 - val_output_loss: 0.4019 - val_aux_output_loss: 0.6156\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4232 - output_loss: 0.4027 - aux_output_loss: 0.6074 - val_loss: 0.4073 - val_output_loss: 0.3879 - val_aux_output_loss: 0.5823\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4177 - output_loss: 0.3995 - aux_output_loss: 0.5812 - val_loss: 0.4048 - val_output_loss: 0.3859 - val_aux_output_loss: 0.5747\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4161 - output_loss: 0.3988 - aux_output_loss: 0.5722 - val_loss: 0.4123 - val_output_loss: 0.3964 - val_aux_output_loss: 0.5557\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3879 - output_loss: 0.3714 - aux_output_loss: 0.5364 - val_loss: 0.3923 - val_output_loss: 0.3756 - val_aux_output_loss: 0.5430\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3975 - output_loss: 0.3806 - aux_output_loss: 0.5490 - val_loss: 0.3843 - val_output_loss: 0.3679 - val_aux_output_loss: 0.5322\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3906 - output_loss: 0.3751 - aux_output_loss: 0.5300 - val_loss: 0.3829 - val_output_loss: 0.3666 - val_aux_output_loss: 0.5292\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3968 - output_loss: 0.3820 - aux_output_loss: 0.5305 - val_loss: 0.3797 - val_output_loss: 0.3637 - val_aux_output_loss: 0.5237\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3656 - output_loss: 0.3505 - aux_output_loss: 0.5012 - val_loss: 0.3840 - val_output_loss: 0.3695 - val_aux_output_loss: 0.5146\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3711 - output_loss: 0.3567 - aux_output_loss: 0.5009 - val_loss: 0.3749 - val_output_loss: 0.3600 - val_aux_output_loss: 0.5088\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3665 - output_loss: 0.3516 - aux_output_loss: 0.5003 - val_loss: 0.3717 - val_output_loss: 0.3574 - val_aux_output_loss: 0.5000\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3643 - output_loss: 0.3490 - aux_output_loss: 0.5020 - val_loss: 0.3732 - val_output_loss: 0.3592 - val_aux_output_loss: 0.4994\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3552 - output_loss: 0.3414 - aux_output_loss: 0.4794 - val_loss: 0.3663 - val_output_loss: 0.3526 - val_aux_output_loss: 0.4893\n",
      "162/162 [==============================] - 0s 852us/step - loss: 0.3542 - output_loss: 0.3422 - aux_output_loss: 0.4625\n",
      "WARNING:tensorflow:7 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000013FC3D901F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name =\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1,name=\"output\")(concat)\n",
    "aux_output = keras.layers.Dense(1,name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs = [input_A,input_B], outputs = [output,aux_output])\n",
    "model.compile(loss = [\"mse\",\"mse\"],loss_weights=[0.9,0.1], optimizer = \"sgd\")\n",
    "history = model.fit([X_train_A, X_train_B], [y_train,y_train], epochs = 20, validation_data=([X_val_A, X_val_B],[y_val,y_val]))\n",
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A,X_test_B], [y_test,y_test])\n",
    "y_pred_main,y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "model.save(\"models/my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 3.0813 - output_1_loss: 2.9865 - output_2_loss: 3.9354 - val_loss: 0.6606 - val_output_1_loss: 0.5650 - val_output_2_loss: 1.5213\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6483 - output_1_loss: 0.5652 - output_2_loss: 1.3958 - val_loss: 0.6856 - val_output_1_loss: 0.6203 - val_output_2_loss: 1.2729\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5627 - output_1_loss: 0.4960 - output_2_loss: 1.1636 - val_loss: 0.7126 - val_output_1_loss: 0.5780 - val_output_2_loss: 1.9245\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5628 - output_1_loss: 0.5062 - output_2_loss: 1.0728 - val_loss: 0.5727 - val_output_1_loss: 0.5226 - val_output_2_loss: 1.0241\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4809 - output_1_loss: 0.4385 - output_2_loss: 0.8627 - val_loss: 0.5262 - val_output_1_loss: 0.4959 - val_output_2_loss: 0.7996\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4740 - output_1_loss: 0.4398 - output_2_loss: 0.7814 - val_loss: 1.2194 - val_output_1_loss: 1.2705 - val_output_2_loss: 0.7599\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4545 - output_1_loss: 0.4267 - output_2_loss: 0.7045 - val_loss: 0.4254 - val_output_1_loss: 0.3984 - val_output_2_loss: 0.6683\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4267 - output_1_loss: 0.4011 - output_2_loss: 0.6568 - val_loss: 0.4221 - val_output_1_loss: 0.3979 - val_output_2_loss: 0.6405\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4148 - output_1_loss: 0.3915 - output_2_loss: 0.6240 - val_loss: 0.4061 - val_output_1_loss: 0.3832 - val_output_2_loss: 0.6119\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3960 - output_1_loss: 0.3749 - output_2_loss: 0.5864 - val_loss: 0.4066 - val_output_1_loss: 0.3852 - val_output_2_loss: 0.5991\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3949 - output_1_loss: 0.3758 - output_2_loss: 0.5669 - val_loss: 0.3997 - val_output_1_loss: 0.3797 - val_output_2_loss: 0.5795\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3986 - output_1_loss: 0.3787 - output_2_loss: 0.5777 - val_loss: 0.3966 - val_output_1_loss: 0.3778 - val_output_2_loss: 0.5658\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4027 - output_1_loss: 0.3840 - output_2_loss: 0.5709 - val_loss: 0.3864 - val_output_1_loss: 0.3680 - val_output_2_loss: 0.5527\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3852 - output_1_loss: 0.3675 - output_2_loss: 0.5446 - val_loss: 0.3822 - val_output_1_loss: 0.3640 - val_output_2_loss: 0.5462\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4659 - output_1_loss: 0.4561 - output_2_loss: 0.5544 - val_loss: 0.3844 - val_output_1_loss: 0.3674 - val_output_2_loss: 0.5377\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3816 - output_1_loss: 0.3650 - output_2_loss: 0.5310 - val_loss: 0.3762 - val_output_1_loss: 0.3591 - val_output_2_loss: 0.5299\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3698 - output_1_loss: 0.3531 - output_2_loss: 0.5204 - val_loss: 0.3850 - val_output_1_loss: 0.3693 - val_output_2_loss: 0.5263\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3816 - output_1_loss: 0.3661 - output_2_loss: 0.5218 - val_loss: 0.3870 - val_output_1_loss: 0.3732 - val_output_2_loss: 0.5114\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3712 - output_1_loss: 0.3558 - output_2_loss: 0.5096 - val_loss: 0.3634 - val_output_1_loss: 0.3474 - val_output_2_loss: 0.5075\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3678 - output_1_loss: 0.3521 - output_2_loss: 0.5096 - val_loss: 0.4227 - val_output_1_loss: 0.4140 - val_output_2_loss: 0.5004\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3768 - output_1_loss: 0.3654 - output_2_loss: 0.4801\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000013FC3D90C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self,units=30, activation= \"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation = activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation = activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self,inputs):\n",
    "        input_A,input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A,hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()\n",
    "model.compile(loss = [\"mse\",\"mse\"],loss_weights=[0.9,0.1], optimizer = \"sgd\")\n",
    "history = model.fit([X_train_A, X_train_B], [y_train,y_train], epochs = 20, validation_data=([X_val_A, X_val_B],[y_val,y_val]))\n",
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A,X_test_B], [y_test,y_test])\n",
    "y_pred_main,y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3190 - output_1_loss: 0.3113 - output_2_loss: 0.3883 - val_loss: 0.3257 - val_output_1_loss: 0.3178 - val_output_2_loss: 0.3962\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3187 - output_1_loss: 0.3110 - output_2_loss: 0.3877 - val_loss: 0.3342 - val_output_1_loss: 0.3264 - val_output_2_loss: 0.4050\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3183 - output_1_loss: 0.3106 - output_2_loss: 0.3872 - val_loss: 0.3356 - val_output_1_loss: 0.3279 - val_output_2_loss: 0.4046\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3183 - output_1_loss: 0.3107 - output_2_loss: 0.3865 - val_loss: 0.3230 - val_output_1_loss: 0.3150 - val_output_2_loss: 0.3944\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3184 - output_1_loss: 0.3108 - output_2_loss: 0.3869 - val_loss: 0.3260 - val_output_1_loss: 0.3183 - val_output_2_loss: 0.3957\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3175 - output_1_loss: 0.3100 - output_2_loss: 0.3845 - val_loss: 0.3302 - val_output_1_loss: 0.3226 - val_output_2_loss: 0.3991\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3159 - output_1_loss: 0.3085 - output_2_loss: 0.3828 - val_loss: 0.3376 - val_output_1_loss: 0.3300 - val_output_2_loss: 0.4061\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3163 - output_1_loss: 0.3089 - output_2_loss: 0.3823 - val_loss: 0.3241 - val_output_1_loss: 0.3166 - val_output_2_loss: 0.3917\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3161 - output_1_loss: 0.3088 - output_2_loss: 0.3821 - val_loss: 0.3237 - val_output_1_loss: 0.3160 - val_output_2_loss: 0.3928\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3150 - output_1_loss: 0.3077 - output_2_loss: 0.3808 - val_loss: 0.3435 - val_output_1_loss: 0.3361 - val_output_2_loss: 0.4102\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3151 - output_1_loss: 0.3079 - output_2_loss: 0.3798 - val_loss: 0.3326 - val_output_1_loss: 0.3254 - val_output_2_loss: 0.3975\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3138 - output_1_loss: 0.3066 - output_2_loss: 0.3789 - val_loss: 0.3195 - val_output_1_loss: 0.3119 - val_output_2_loss: 0.3878\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3150 - output_1_loss: 0.3078 - output_2_loss: 0.3795 - val_loss: 0.3336 - val_output_1_loss: 0.3258 - val_output_2_loss: 0.4036\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3142 - output_1_loss: 0.3070 - output_2_loss: 0.3788 - val_loss: 0.3385 - val_output_1_loss: 0.3306 - val_output_2_loss: 0.4091\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3139 - output_1_loss: 0.3067 - output_2_loss: 0.3785 - val_loss: 0.3554 - val_output_1_loss: 0.3475 - val_output_2_loss: 0.4260\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3126 - output_1_loss: 0.3055 - output_2_loss: 0.3764 - val_loss: 0.3299 - val_output_1_loss: 0.3223 - val_output_2_loss: 0.3975\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3152 - output_1_loss: 0.3081 - output_2_loss: 0.3788 - val_loss: 0.3297 - val_output_1_loss: 0.3227 - val_output_2_loss: 0.3930\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3119 - output_1_loss: 0.3048 - output_2_loss: 0.3760 - val_loss: 0.3188 - val_output_1_loss: 0.3116 - val_output_2_loss: 0.3835\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3113 - output_1_loss: 0.3042 - output_2_loss: 0.3752 - val_loss: 0.3352 - val_output_1_loss: 0.3282 - val_output_2_loss: 0.3979\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3110 - output_1_loss: 0.3039 - output_2_loss: 0.3747 - val_loss: 0.3459 - val_output_1_loss: 0.3389 - val_output_2_loss: 0.4085\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"models/my_keras_model.h5\", save_best_only = True)\n",
    "history = model.fit([X_train_A, X_train_B], [y_train,y_train], epochs = 20,\n",
    "                    validation_data=([X_val_A, X_val_B],[y_val,y_val]),\n",
    "                    callbacks = [checkpoint_cb]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3676 - output_loss: 0.3543 - aux_output_loss: 0.4874 - val_loss: 0.3659 - val_output_loss: 0.3510 - val_aux_output_loss: 0.4993\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3644 - output_loss: 0.3512 - aux_output_loss: 0.4840 - val_loss: 0.3932 - val_output_loss: 0.3819 - val_aux_output_loss: 0.4951\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3553 - output_loss: 0.3423 - aux_output_loss: 0.4721 - val_loss: 0.3630 - val_output_loss: 0.3499 - val_aux_output_loss: 0.4804\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3595 - output_loss: 0.3473 - aux_output_loss: 0.4690 - val_loss: 0.3821 - val_output_loss: 0.3699 - val_aux_output_loss: 0.4924\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3536 - output_loss: 0.3412 - aux_output_loss: 0.4651 - val_loss: 0.3545 - val_output_loss: 0.3418 - val_aux_output_loss: 0.4687\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3504 - output_loss: 0.3382 - aux_output_loss: 0.4610 - val_loss: 0.3482 - val_output_loss: 0.3354 - val_aux_output_loss: 0.4637\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3544 - output_loss: 0.3427 - aux_output_loss: 0.4594 - val_loss: 0.3543 - val_output_loss: 0.3416 - val_aux_output_loss: 0.4678\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3473 - output_loss: 0.3353 - aux_output_loss: 0.4554 - val_loss: 0.3463 - val_output_loss: 0.3337 - val_aux_output_loss: 0.4596\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3440 - output_loss: 0.3321 - aux_output_loss: 0.4511 - val_loss: 0.3470 - val_output_loss: 0.3349 - val_aux_output_loss: 0.4553\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3452 - output_loss: 0.3338 - aux_output_loss: 0.4483 - val_loss: 0.3546 - val_output_loss: 0.3435 - val_aux_output_loss: 0.4545\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3468 - output_loss: 0.3357 - aux_output_loss: 0.4466 - val_loss: 0.3482 - val_output_loss: 0.3364 - val_aux_output_loss: 0.4540\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3421 - output_loss: 0.3308 - aux_output_loss: 0.4442 - val_loss: 0.3445 - val_output_loss: 0.3326 - val_aux_output_loss: 0.4515\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3399 - output_loss: 0.3286 - aux_output_loss: 0.4419 - val_loss: 0.3522 - val_output_loss: 0.3410 - val_aux_output_loss: 0.4528\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3373 - output_loss: 0.3261 - aux_output_loss: 0.4387 - val_loss: 0.3477 - val_output_loss: 0.3365 - val_aux_output_loss: 0.4486\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3398 - output_loss: 0.3290 - aux_output_loss: 0.4372 - val_loss: 0.3611 - val_output_loss: 0.3512 - val_aux_output_loss: 0.4504\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3387 - output_loss: 0.3277 - aux_output_loss: 0.4376 - val_loss: 0.3382 - val_output_loss: 0.3268 - val_aux_output_loss: 0.4407\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3363 - output_loss: 0.3254 - aux_output_loss: 0.4337 - val_loss: 0.3477 - val_output_loss: 0.3367 - val_aux_output_loss: 0.4462\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3349 - output_loss: 0.3242 - aux_output_loss: 0.4315 - val_loss: 0.3381 - val_output_loss: 0.3271 - val_aux_output_loss: 0.4377\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3333 - output_loss: 0.3227 - aux_output_loss: 0.4289 - val_loss: 0.3612 - val_output_loss: 0.3512 - val_aux_output_loss: 0.4514\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3395 - output_loss: 0.3295 - aux_output_loss: 0.4293 - val_loss: 0.3441 - val_output_loss: 0.3336 - val_aux_output_loss: 0.4387\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3336 - output_loss: 0.3232 - aux_output_loss: 0.4276 - val_loss: 0.3411 - val_output_loss: 0.3302 - val_aux_output_loss: 0.4395\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3306 - output_loss: 0.3202 - aux_output_loss: 0.4241 - val_loss: 0.3404 - val_output_loss: 0.3296 - val_aux_output_loss: 0.4374\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3323 - output_loss: 0.3222 - aux_output_loss: 0.4232 - val_loss: 0.3524 - val_output_loss: 0.3437 - val_aux_output_loss: 0.4311\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3419 - output_loss: 0.3330 - aux_output_loss: 0.4220 - val_loss: 0.3409 - val_output_loss: 0.3312 - val_aux_output_loss: 0.4274\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3308 - output_loss: 0.3209 - aux_output_loss: 0.4207 - val_loss: 0.3430 - val_output_loss: 0.3337 - val_aux_output_loss: 0.4265\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3301 - output_loss: 0.3204 - aux_output_loss: 0.4177 - val_loss: 0.3372 - val_output_loss: 0.3276 - val_aux_output_loss: 0.4244\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3299 - output_loss: 0.3205 - aux_output_loss: 0.4144 - val_loss: 0.3361 - val_output_loss: 0.3266 - val_aux_output_loss: 0.4220\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3283 - output_loss: 0.3187 - aux_output_loss: 0.4147 - val_loss: 0.3312 - val_output_loss: 0.3214 - val_aux_output_loss: 0.4195\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3285 - output_loss: 0.3191 - aux_output_loss: 0.4125 - val_loss: 0.3365 - val_output_loss: 0.3270 - val_aux_output_loss: 0.4220\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - output_loss: 0.3166 - aux_output_loss: 0.4112 - val_loss: 0.3403 - val_output_loss: 0.3308 - val_aux_output_loss: 0.4259\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3268 - output_loss: 0.3176 - aux_output_loss: 0.4093 - val_loss: 0.3290 - val_output_loss: 0.3193 - val_aux_output_loss: 0.4169\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3309 - output_loss: 0.3222 - aux_output_loss: 0.4092 - val_loss: 0.3362 - val_output_loss: 0.3271 - val_aux_output_loss: 0.4183\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3240 - output_loss: 0.3149 - aux_output_loss: 0.4058 - val_loss: 0.3723 - val_output_loss: 0.3672 - val_aux_output_loss: 0.4180\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3287 - output_loss: 0.3200 - aux_output_loss: 0.4066 - val_loss: 0.3778 - val_output_loss: 0.3715 - val_aux_output_loss: 0.4347\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3275 - output_loss: 0.3190 - aux_output_loss: 0.4038 - val_loss: 0.3504 - val_output_loss: 0.3424 - val_aux_output_loss: 0.4224\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3241 - output_loss: 0.3152 - aux_output_loss: 0.4041 - val_loss: 0.3451 - val_output_loss: 0.3364 - val_aux_output_loss: 0.4230\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3287 - output_loss: 0.3203 - aux_output_loss: 0.4045 - val_loss: 0.3623 - val_output_loss: 0.3558 - val_aux_output_loss: 0.4208\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3221 - output_loss: 0.3132 - aux_output_loss: 0.4019 - val_loss: 0.3457 - val_output_loss: 0.3384 - val_aux_output_loss: 0.4109\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3242 - output_loss: 0.3155 - aux_output_loss: 0.4017 - val_loss: 0.3479 - val_output_loss: 0.3406 - val_aux_output_loss: 0.4138\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3228 - output_loss: 0.3141 - aux_output_loss: 0.4003 - val_loss: 0.3414 - val_output_loss: 0.3339 - val_aux_output_loss: 0.4084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - output_loss: 0.3180 - aux_output_loss: 0.3991 - val_loss: 0.3526 - val_output_loss: 0.3460 - val_aux_output_loss: 0.4124\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/my_keras_model.h5\")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit([X_train_A, X_train_B], [y_train,y_train], epochs = 100,\n",
    "                    validation_data=([X_val_A, X_val_B],[y_val,y_val]),\n",
    "                    callbacks = [checkpoint_cb, early_stopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatio(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"]/logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3155 - output_loss: 0.3078 - aux_output_loss: 0.3850 - val_loss: 0.3247 - val_output_loss: 0.3170 - val_aux_output_loss: 0.3938\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3122 - output_loss: 0.3044 - aux_output_loss: 0.3829 - val_loss: 0.3325 - val_output_loss: 0.3259 - val_aux_output_loss: 0.3918\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3122 - output_loss: 0.3046 - aux_output_loss: 0.3809 - val_loss: 0.3556 - val_output_loss: 0.3510 - val_aux_output_loss: 0.3971\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3120 - output_loss: 0.3044 - aux_output_loss: 0.3801 - val_loss: 0.3643 - val_output_loss: 0.3596 - val_aux_output_loss: 0.4066\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3139 - output_loss: 0.3064 - aux_output_loss: 0.3820 - val_loss: 0.3504 - val_output_loss: 0.3443 - val_aux_output_loss: 0.40590s - loss: 0.3108 - output_loss: 0.3033 - aux_output_loss: 0.3\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3100 - output_loss: 0.3022 - aux_output_loss: 0.3802 - val_loss: 0.3158 - val_output_loss: 0.3079 - val_aux_output_loss: 0.3865\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3121 - output_loss: 0.3045 - aux_output_loss: 0.3801 - val_loss: 0.3176 - val_output_loss: 0.3099 - val_aux_output_loss: 0.3866\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3092 - output_loss: 0.3015 - aux_output_loss: 0.3779 - val_loss: 0.3220 - val_output_loss: 0.3141 - val_aux_output_loss: 0.3935\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3090 - output_loss: 0.3013 - aux_output_loss: 0.3781 - val_loss: 0.3203 - val_output_loss: 0.3128 - val_aux_output_loss: 0.3879\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3081 - output_loss: 0.3004 - aux_output_loss: 0.3777 - val_loss: 0.3254 - val_output_loss: 0.3180 - val_aux_output_loss: 0.3923\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3082 - output_loss: 0.3005 - aux_output_loss: 0.3773 - val_loss: 0.3187 - val_output_loss: 0.3115 - val_aux_output_loss: 0.3836\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3093 - output_loss: 0.3019 - aux_output_loss: 0.3756 - val_loss: 0.3305 - val_output_loss: 0.3239 - val_aux_output_loss: 0.3905\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3084 - output_loss: 0.3007 - aux_output_loss: 0.3768 - val_loss: 0.3201 - val_output_loss: 0.3127 - val_aux_output_loss: 0.3871\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3088 - output_loss: 0.3013 - aux_output_loss: 0.3764 - val_loss: 0.3220 - val_output_loss: 0.3153 - val_aux_output_loss: 0.3824\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3085 - output_loss: 0.3012 - aux_output_loss: 0.3747 - val_loss: 0.3228 - val_output_loss: 0.3155 - val_aux_output_loss: 0.3886\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3078 - output_loss: 0.3003 - aux_output_loss: 0.3752 - val_loss: 0.3122 - val_output_loss: 0.3046 - val_aux_output_loss: 0.3803\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3055 - output_loss: 0.2981 - aux_output_loss: 0.3723 - val_loss: 0.3243 - val_output_loss: 0.3169 - val_aux_output_loss: 0.3912\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3075 - output_loss: 0.3001 - aux_output_loss: 0.3735 - val_loss: 0.3166 - val_output_loss: 0.3089 - val_aux_output_loss: 0.3854\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3063 - output_loss: 0.2989 - aux_output_loss: 0.3724 - val_loss: 0.3160 - val_output_loss: 0.3089 - val_aux_output_loss: 0.3796\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3073 - output_loss: 0.2998 - aux_output_loss: 0.3740 - val_loss: 0.3124 - val_output_loss: 0.3046 - val_aux_output_loss: 0.3824\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3046 - output_loss: 0.2973 - aux_output_loss: 0.3707 - val_loss: 0.3198 - val_output_loss: 0.3126 - val_aux_output_loss: 0.3852\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3044 - output_loss: 0.2970 - aux_output_loss: 0.3712 - val_loss: 0.3097 - val_output_loss: 0.3021 - val_aux_output_loss: 0.3783\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3044 - output_loss: 0.2969 - aux_output_loss: 0.3718 - val_loss: 0.3209 - val_output_loss: 0.3143 - val_aux_output_loss: 0.3802\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3034 - output_loss: 0.2961 - aux_output_loss: 0.3697 - val_loss: 0.3350 - val_output_loss: 0.3289 - val_aux_output_loss: 0.3893\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3045 - output_loss: 0.2973 - aux_output_loss: 0.3694 - val_loss: 0.3158 - val_output_loss: 0.3085 - val_aux_output_loss: 0.3820\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3014 - output_loss: 0.2941 - aux_output_loss: 0.3672 - val_loss: 0.3171 - val_output_loss: 0.3100 - val_aux_output_loss: 0.3807\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3027 - output_loss: 0.2954 - aux_output_loss: 0.3687 - val_loss: 0.3243 - val_output_loss: 0.3179 - val_aux_output_loss: 0.3813\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3029 - output_loss: 0.2956 - aux_output_loss: 0.3689 - val_loss: 0.3297 - val_output_loss: 0.3237 - val_aux_output_loss: 0.3833\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3023 - output_loss: 0.2950 - aux_output_loss: 0.3677 - val_loss: 0.3323 - val_output_loss: 0.3266 - val_aux_output_loss: 0.3836\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3006 - output_loss: 0.2934 - aux_output_loss: 0.3653 - val_loss: 0.3335 - val_output_loss: 0.3268 - val_aux_output_loss: 0.3935\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "root_logdir = os.path.join(os.curdir,\"my_logs\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit([X_train_A, X_train_B],\n",
    "                    [y_train,y_train],\n",
    "                    epochs = 30,\n",
    "                    validation_data=(\n",
    "                        [X_val_A, X_val_B],\n",
    "                        [y_val,y_val]\n",
    "                    ),\n",
    "                    callbacks=[tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000+1):\n",
    "        tf.summary.scalar(\"my_scalar\",np.sin(step/10), step = step)\n",
    "        data = (np.random.randn(100)+2 )* step/100\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50,step=step)\n",
    "        images = np.random.rand(2,32,32,3)\n",
    "        tf.summary.image(\"my_images\", images*step/1000, step = step)\n",
    "        texts = [\"Step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000)/48000*2 * np.pi)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1,-1,1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate = 48000, step=step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    " def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape = 8):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "        for layer in range(n_hidden):\n",
    "            model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer = optimizer)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.1854 - val_loss: 0.9148\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6570 - val_loss: 0.6392\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.5561\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.5077\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5149 - val_loss: 0.4840\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.4594\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4587 - val_loss: 0.4496\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 0.4359\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.4504 - val_loss: 0.4394\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4277 - val_loss: 0.4219\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4217\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.4072\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.4039\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 939us/step - loss: 0.4078 - val_loss: 0.3988\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.3963\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.3931\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3886 - val_loss: 0.3900\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.3885\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.3845\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.3855\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.3803\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.3799\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.3792\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.3766\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3819 - val_loss: 0.3728\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3762 - val_loss: 0.3764\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.3769\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3713\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3687\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3664 - val_loss: 0.3714\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.3664\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3710\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.3720\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3664 - val_loss: 0.3645\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.3632\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.3646\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3736\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3633\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.3611\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3522 - val_loss: 0.3618\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.3614\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3579\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.3599\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3704 - val_loss: 0.3584\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3602\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3595\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3554\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3609\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.3559\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3573\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.3572\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3540\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3555\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3531\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3574\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3516\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3514\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3547\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3516\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3571\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3502 - val_loss: 0.3581\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3538\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 0.3563\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3533\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.3514\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.3521\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.3487\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3459\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3584\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3472\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.3493\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.3452\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.3469\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.3458\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3427 - val_loss: 0.3470\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 970us/step - loss: 0.3394 - val_loss: 0.3491\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3437\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3723\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3441\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.3433\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3312 - val_loss: 0.3423\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.3491\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3412 - val_loss: 0.3418\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1000us/step - loss: 0.3426 - val_loss: 0.3425\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.3492\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3420\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 965us/step - loss: 0.3274 - val_loss: 0.3565\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3377 - val_loss: 0.3563\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3395 - val_loss: 0.3451\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3469\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1000us/step - loss: 0.3407 - val_loss: 0.3403\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3460\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3483\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3342 - val_loss: 0.3452\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3364 - val_loss: 0.3409\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 927us/step - loss: 0.3306 - val_loss: 0.3447\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.3301 - val_loss: 0.3399\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3436\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.3396\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.3398\n",
      "162/162 [==============================] - 0s 680us/step - loss: 0.3337\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000013FC1FB7B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train,y_train,epochs=100,validation_data = (X_val,y_val), callbacks = [keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test,y_test)\n",
    "y_pred=keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.6483 - val_loss: 0.9121\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6955 - val_loss: 0.7170\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6249 - val_loss: 0.6318\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5790 - val_loss: 0.8951\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5438 - val_loss: 0.7249\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5942 - val_loss: 0.7942\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5204 - val_loss: 0.9486\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5907 - val_loss: 1.2411\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5126 - val_loss: 0.9605\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4885 - val_loss: 0.7043\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5511 - val_loss: 0.5709\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4781 - val_loss: 0.5124\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4835 - val_loss: 0.4793\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4546 - val_loss: 0.4777\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4418\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4405\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4360\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4384\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4277\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4282\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.4224\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4313 - val_loss: 0.4138\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4051 - val_loss: 0.4209\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.4199\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4098\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4046\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.4042\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4134 - val_loss: 0.3989\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.3986\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4004\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.3970\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.4034\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3728 - val_loss: 0.3984\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4150 - val_loss: 0.3894\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.3891\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.399 - 0s 1ms/step - loss: 0.3979 - val_loss: 0.3849\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 0.3853\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3809 - val_loss: 0.3848\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3796 - val_loss: 0.3865\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.3855\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3732 - val_loss: 0.3769\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.3783\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.3852\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.3791\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.3923\n",
      "Epoch 46/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3969\n",
      "Epoch 47/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.3791\n",
      "Epoch 48/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.3766\n",
      "Epoch 49/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.3743\n",
      "Epoch 50/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3696\n",
      "Epoch 51/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3810\n",
      "Epoch 52/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.3685\n",
      "Epoch 53/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.3668\n",
      "Epoch 54/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3666 - val_loss: 0.3841\n",
      "Epoch 55/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3719 - val_loss: 0.3714\n",
      "Epoch 56/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3575 - val_loss: 0.3637\n",
      "Epoch 57/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.3738\n",
      "Epoch 58/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3879\n",
      "Epoch 59/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.3726\n",
      "Epoch 60/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.342 - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3689\n",
      "121/121 [==============================] - 0s 771us/step - loss: 0.3596\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.3148 - val_loss: 0.7470\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6806 - val_loss: 0.6242\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6180 - val_loss: 0.6759\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5584 - val_loss: 0.5403\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5110 - val_loss: 0.6101\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 0.5287\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4849 - val_loss: 0.6530\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4780 - val_loss: 0.9668\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4781 - val_loss: 1.0207\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4860 - val_loss: 0.9556\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4844 - val_loss: 1.0249\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.8758\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4681 - val_loss: 0.8221\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5879 - val_loss: 0.7867\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5046 - val_loss: 0.6598\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4860 - val_loss: 0.5431\n",
      "121/121 [==============================] - 0s 671us/step - loss: 0.4455\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5186 - val_loss: 1.7127\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7177 - val_loss: 1.0620\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6154 - val_loss: 0.6925\n",
      "Epoch 4/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5740 - val_loss: 0.5240\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5345 - val_loss: 0.5004\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4999 - val_loss: 0.5428\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5262 - val_loss: 0.6158\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4810 - val_loss: 0.6979\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4609 - val_loss: 0.7777\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4512 - val_loss: 0.8627\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4591 - val_loss: 0.9186\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4486 - val_loss: 0.9846\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4721 - val_loss: 1.0189\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4519 - val_loss: 1.0518\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 1.0912\n",
      "121/121 [==============================] - 0s 586us/step - loss: 0.5850\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.1155 - val_loss: 0.6564\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6401 - val_loss: 0.5559\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5543 - val_loss: 0.5039\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4845 - val_loss: 0.4668\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4730 - val_loss: 0.4381\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4203\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4053\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.3945\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4007 - val_loss: 0.3862\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3879 - val_loss: 0.3816\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.3793\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.3725\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.3674\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.3637\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.3624\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.3809\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3771 - val_loss: 0.3640\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3520\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3582\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 0.3576\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3580 - val_loss: 0.3497\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3535\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3377 - val_loss: 0.3503\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3465\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3407\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.3431\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3379 - val_loss: 0.3420\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3463\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3440\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3408 - val_loss: 0.3479\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3341\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.3363\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3271 - val_loss: 0.3338\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3309\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3398\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3417\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3269\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3364\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3088 - val_loss: 0.3355\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3343\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3215 - val_loss: 0.3393\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3291\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3198 - val_loss: 0.3225\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3207 - val_loss: 0.3230\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3227\n",
      "Epoch 46/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3309\n",
      "Epoch 47/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3083 - val_loss: 0.3331\n",
      "Epoch 48/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3135 - val_loss: 0.3290\n",
      "Epoch 49/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3040 - val_loss: 0.3352\n",
      "Epoch 50/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3152 - val_loss: 0.3286\n",
      "Epoch 51/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3096 - val_loss: 0.3244\n",
      "Epoch 52/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3194 - val_loss: 0.3180\n",
      "Epoch 53/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3131 - val_loss: 0.3163\n",
      "Epoch 54/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.3282\n",
      "Epoch 55/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3078 - val_loss: 0.3175\n",
      "Epoch 56/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2875 - val_loss: 0.3206\n",
      "Epoch 57/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2941 - val_loss: 0.3188\n",
      "Epoch 58/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2949 - val_loss: 0.3278\n",
      "Epoch 59/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3010 - val_loss: 0.3168\n",
      "Epoch 60/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2974 - val_loss: 0.3188\n",
      "121/121 [==============================] - 0s 644us/step - loss: 0.3061\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.0820 - val_loss: 0.6359\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5682 - val_loss: 0.5729\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 0.5192\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4903 - val_loss: 0.4683\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4366\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4137 - val_loss: 0.4342\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.3935\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3904 - val_loss: 0.4096\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3723\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3567 - val_loss: 0.3659\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3693\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3624\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3657\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3632\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 0.3755\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3495\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.3530\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3649\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3898\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.4204\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3296 - val_loss: 0.3758\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3259 - val_loss: 0.3480\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3634\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.3685\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3516\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3011 - val_loss: 0.3530\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3365\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3242 - val_loss: 0.3355\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3247 - val_loss: 0.3351\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3073 - val_loss: 0.3405\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3340 - val_loss: 0.3364\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3408 - val_loss: 0.3336\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3093 - val_loss: 0.3319\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3077 - val_loss: 0.3414\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3210 - val_loss: 0.3276\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.3487\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3337\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3002 - val_loss: 0.3279\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3098 - val_loss: 0.3337\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3404\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.4193\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3287\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3042 - val_loss: 0.3494\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3674\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3022 - val_loss: 0.4188\n",
      "121/121 [==============================] - 0s 640us/step - loss: 0.3609\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.4548 - val_loss: 1.2854\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7569 - val_loss: 0.7816\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6493 - val_loss: 0.6005\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6147 - val_loss: 0.5443\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5510 - val_loss: 0.5364\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 0.5455\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5109 - val_loss: 0.5773\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4986 - val_loss: 0.6078\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4575 - val_loss: 0.6227\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.6370\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4326 - val_loss: 0.6661\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.6386\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.6325\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.6046\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3753 - val_loss: 0.5844\n",
      "121/121 [==============================] - 0s 639us/step - loss: 0.4328\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9715 - val_loss: 18.6888\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6885 - val_loss: 0.6456\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4949 - val_loss: 0.4310\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.3850\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.4071\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.400 - 0s 1ms/step - loss: 0.4000 - val_loss: 0.3781\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3832 - val_loss: 0.3727\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.3673\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.3715\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.3545\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.3684\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.3608\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4886 - val_loss: 0.3491\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3500\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3715\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3439\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3627\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3474\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3364\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3593 - val_loss: 0.3402\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3324 - val_loss: 0.3764\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3404\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3477 - val_loss: 0.3556\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3321\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3265 - val_loss: 0.3278\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3352 - val_loss: 0.3405\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.3304\n",
      "Epoch 28/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3164 - val_loss: 0.3548\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3300 - val_loss: 0.3357\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3256\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3313\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.3387\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3259\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.3142\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3189 - val_loss: 0.3172\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3192 - val_loss: 0.3234\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3065 - val_loss: 0.3188\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3095 - val_loss: 0.3234\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.3120\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3129\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3129 - val_loss: 0.3158\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2954 - val_loss: 0.3249\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3041 - val_loss: 0.3392\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3188 - val_loss: 0.3213\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3374\n",
      "Epoch 46/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.3136\n",
      "Epoch 47/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3086 - val_loss: 0.3166\n",
      "Epoch 48/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3091 - val_loss: 0.3246\n",
      "Epoch 49/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3012 - val_loss: 0.3082\n",
      "Epoch 50/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2922 - val_loss: 0.3181\n",
      "Epoch 51/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3077 - val_loss: 0.3039\n",
      "Epoch 52/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.3134\n",
      "Epoch 53/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.3179\n",
      "Epoch 54/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3083 - val_loss: 0.3239\n",
      "Epoch 55/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3056 - val_loss: 0.3140\n",
      "Epoch 56/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.3101\n",
      "Epoch 57/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2938 - val_loss: 0.3205\n",
      "Epoch 58/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3042 - val_loss: 0.3066\n",
      "Epoch 59/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3007 - val_loss: 0.3056\n",
      "Epoch 60/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2951 - val_loss: 0.3050\n",
      "121/121 [==============================] - 0s 763us/step - loss: 0.3184\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.7637 - val_loss: 55.3004\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 900us/step - loss: nan\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0105 - val_loss: 0.8027\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5013 - val_loss: 0.8477\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.6178\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.5059\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4417\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.4235\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3989 - val_loss: 0.3987\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.4552\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.5340\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.5621\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4016 - val_loss: 0.4722\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.4387\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.5200\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3794 - val_loss: 0.6601\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.7631\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3630 - val_loss: 0.6453\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3618 - val_loss: 0.7298\n",
      "121/121 [==============================] - 0s 619us/step - loss: 0.4302\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9548 - val_loss: 2.4056\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6300 - val_loss: 21.2474\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9179 - val_loss: 0.3946\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.3811\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3917 - val_loss: 0.3820\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.3520\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3611\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.3729\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.3491\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3517 - val_loss: 0.3558\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.3577\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3410 - val_loss: 0.3468\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3417 - val_loss: 0.3480\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3386 - val_loss: 0.3385\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.3302\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3330\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3271\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.3283\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3285\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3083 - val_loss: 0.3309\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3297 - val_loss: 0.3240\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3242\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.3169\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3280\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3188 - val_loss: 0.3243\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3264\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3218 - val_loss: 0.3201\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3238\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3197\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3157 - val_loss: 0.3242\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.3396\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.3180\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.317 - 0s 1ms/step - loss: 0.3178 - val_loss: 0.3249\n",
      "121/121 [==============================] - 0s 634us/step - loss: 0.3266\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 598us/step - loss: nan\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4001 - val_loss: 0.5401\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4946 - val_loss: 1.1529\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 1.0483\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.9127\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 0.6771\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4891\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.4227\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.3642\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.3835\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3733 - val_loss: 0.4154\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.5071\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.5604\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.5717\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.8182\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.7130\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3438 - val_loss: 0.6065\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4082 - val_loss: 0.7497\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.8527\n",
      "121/121 [==============================] - 0s 742us/step - loss: 0.4786\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1170 - val_loss: 19.6245\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 710us/step - loss: nan\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 706us/step - loss: nan\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1712 - val_loss: 1.9521\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 2.3052\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 1.7704\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 1.5417\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4239 - val_loss: 0.9411\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.6741\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4137 - val_loss: 0.4347\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3874 - val_loss: 0.3981\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.5896\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4308 - val_loss: 0.4182\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3704 - val_loss: 0.4237\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3769 - val_loss: 0.4998\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3554 - val_loss: 0.5192\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.6069\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.6620\n",
      "Epoch 16/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.6917\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.7696\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.7147\n",
      "121/121 [==============================] - 0s 620us/step - loss: 0.4460\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.2848 - val_loss: 0.6344\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5973 - val_loss: 0.5523\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.5048\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.4748\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4774 - val_loss: 0.4542\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4460 - val_loss: 0.4336\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4159 - val_loss: 0.4200\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4115\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.4069\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4162 - val_loss: 0.3961\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.3918\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.3868\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3959 - val_loss: 0.3826\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3802\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.3718\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3700\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3629 - val_loss: 0.3673\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.3680\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.3600\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.3619\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3566 - val_loss: 0.3628\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3544\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.3558\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3511\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.3481\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3543\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3356 - val_loss: 0.3619\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3504\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3368 - val_loss: 0.3463\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3417\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3465\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3398\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3409\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3377 - val_loss: 0.3446\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3282 - val_loss: 0.3409\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.3418\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3366\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3360\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.340 - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3343\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3325 - val_loss: 0.3368\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.3380\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.337 - 0s 1ms/step - loss: 0.3364 - val_loss: 0.3356\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3218 - val_loss: 0.3351\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.3300\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3052 - val_loss: 0.3284\n",
      "Epoch 46/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3173 - val_loss: 0.3259\n",
      "Epoch 47/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.3238\n",
      "Epoch 48/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.3358\n",
      "Epoch 49/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3351 - val_loss: 0.3307\n",
      "Epoch 50/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.3252\n",
      "Epoch 51/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3151 - val_loss: 0.3246\n",
      "Epoch 52/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.3252\n",
      "Epoch 53/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3191\n",
      "Epoch 54/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3049 - val_loss: 0.3407\n",
      "Epoch 55/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3346 - val_loss: 0.3218\n",
      "Epoch 56/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3194 - val_loss: 0.3194\n",
      "Epoch 57/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3179\n",
      "Epoch 58/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3078 - val_loss: 0.3239\n",
      "Epoch 59/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.3190\n",
      "Epoch 60/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3206\n",
      "121/121 [==============================] - 0s 639us/step - loss: 0.3221\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.0699 - val_loss: 0.7739\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6897 - val_loss: 0.6221\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6170 - val_loss: 0.5598\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5534 - val_loss: 0.5193\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5061 - val_loss: 0.4838\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4867 - val_loss: 0.4600\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4614 - val_loss: 0.4445\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4250\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4110\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4107 - val_loss: 0.4054\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.3943\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.3891\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.3873\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.3803\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.3744\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.3723\n",
      "Epoch 17/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3794 - val_loss: 0.3676\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.3650\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3663\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.3678\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.3588\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.3586\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3572\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3557\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.3596\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3389 - val_loss: 0.3599\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.3536\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3314 - val_loss: 0.3570\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3577\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3587\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3364 - val_loss: 0.3474\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3380 - val_loss: 0.3441\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3310 - val_loss: 0.3519\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3445\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3460\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3397\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3354 - val_loss: 0.3443\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3404\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3117 - val_loss: 0.3383\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.3688\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3553\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3173 - val_loss: 0.3427\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2968 - val_loss: 0.3330\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3079 - val_loss: 0.3391\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3509\n",
      "Epoch 46/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3014 - val_loss: 0.3470\n",
      "Epoch 47/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3075 - val_loss: 0.3536\n",
      "Epoch 48/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3041 - val_loss: 0.3638\n",
      "Epoch 49/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3173 - val_loss: 0.3617\n",
      "Epoch 50/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3232 - val_loss: 0.3443\n",
      "Epoch 51/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 0.3410\n",
      "Epoch 52/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3051 - val_loss: 0.3251\n",
      "Epoch 53/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3250\n",
      "Epoch 54/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2975 - val_loss: 0.3300\n",
      "Epoch 55/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.3349\n",
      "Epoch 56/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2954 - val_loss: 0.3325\n",
      "Epoch 57/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2896 - val_loss: 0.3281\n",
      "Epoch 58/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2931 - val_loss: 0.3225\n",
      "Epoch 59/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3013 - val_loss: 0.3205\n",
      "Epoch 60/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2912 - val_loss: 0.3217\n",
      "121/121 [==============================] - 0s 588us/step - loss: 0.3498\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.2029 - val_loss: 0.6794\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6625 - val_loss: 0.5699\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5696 - val_loss: 0.5561\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5119 - val_loss: 0.5535\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4758 - val_loss: 0.5982\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.6194\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4575 - val_loss: 0.6361\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3969 - val_loss: 0.6310\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.6278\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4059 - val_loss: 0.6321\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3946 - val_loss: 0.5964\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.5678\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.5660\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.5367\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.5043\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.4833\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.4683\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.4504\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.4407\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.4127\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3401 - val_loss: 0.3985\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3951\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.3957\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3682\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3322 - val_loss: 0.3583\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3547\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3493\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3393\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3457\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3381\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3397\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3364\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3434\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3126 - val_loss: 0.3426\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.3485\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3330 - val_loss: 0.3662\n",
      "Epoch 37/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3056 - val_loss: 0.3623\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3237 - val_loss: 0.3756\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.4019\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3279 - val_loss: 0.4019\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.4094\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3097 - val_loss: 0.4164\n",
      "121/121 [==============================] - 0s 698us/step - loss: 0.3421\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.7471 - val_loss: 29.1239\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 17.1740 - val_loss: 4722.9941\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3998.1613 - val_loss: 1126581.2500\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1490416.0684 - val_loss: 281771616.0000\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 7304130.5849 - val_loss: 67240673280.0000\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 8361520089.3827 - val_loss: 16621144899584.0000\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 617829961867.0618 - val_loss: 4153428527808512.0000\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4237415777383327.0000 - val_loss: 1024537372986966016.0000\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 69970781087456840.0000 - val_loss: 247777971175321239552.0000\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 10048762895731429376.0000 - val_loss: 61517873873188292657152.0000\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 66554923644732517122048.0000 - val_loss: 15208450433371968821526528.0000\n",
      "121/121 [==============================] - 0s 605us/step - loss: 28901184075905307246592.0000\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4499 - val_loss: 1109.1393\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1539.5640 - val_loss: 471756.4375\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1159724.2354 - val_loss: 209576192.0000\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5051118.0891 - val_loss: 90785054720.0000\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 249670976355.3251 - val_loss: 38919030702080.0000\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 196210306381647.0000 - val_loss: 16783765825126400.0000\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1560692470643927.0000 - val_loss: 7210359210687594496.0000\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 57548000824620072960.0000 - val_loss: 3206952496055560699904.0000\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6978110225127488618496.0000 - val_loss: 1373606389144097755496448.0000\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 20413208811818596100472832.0000 - val_loss: 606255490609421386740924416.0000\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2126471060637367825768382464.0000 - val_loss: 261372085604992399436920389632.0000\n",
      "121/121 [==============================] - 0s 624us/step - loss: 1327197783326004618436018176.0000\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.6218 - val_loss: 3.8024\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6373 - val_loss: 4.4682\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5567 - val_loss: 8.0068\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6848 - val_loss: 6.8144\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5647 - val_loss: 7.1275\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5054 - val_loss: 6.7910\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5643 - val_loss: 7.9562\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5120 - val_loss: 6.8408\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5040 - val_loss: 6.9058\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 7.0218\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5052 - val_loss: 6.9109\n",
      "121/121 [==============================] - 0s 593us/step - loss: 2.3489\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.5886 - val_loss: 2.3778\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0192 - val_loss: 1.0825\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7570 - val_loss: 0.7789\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6605 - val_loss: 0.6584\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6282 - val_loss: 0.6108\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6326 - val_loss: 0.5940\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5905 - val_loss: 0.5970\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6102 - val_loss: 0.5723\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5847 - val_loss: 0.5531\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.5510\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5555 - val_loss: 0.5678\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5564 - val_loss: 0.5645\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.5498 - val_loss: 0.5518\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.5386 - val_loss: 0.5380\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5249 - val_loss: 0.5587\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.5547\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5217 - val_loss: 0.5732\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.5468\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.5627\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5121 - val_loss: 0.5635\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5473 - val_loss: 0.5331\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.5578\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5271 - val_loss: 0.5534\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.5258 - val_loss: 0.5425\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.5370 - val_loss: 0.5521\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5313 - val_loss: 0.5439\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.5520 - val_loss: 0.5335\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.516 - 0s 1ms/step - loss: 0.5164 - val_loss: 0.5387\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.5369\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5302 - val_loss: 0.5569\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5178 - val_loss: 0.5616\n",
      "121/121 [==============================] - 0s 611us/step - loss: 0.5455\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 4.9985 - val_loss: 1.1588\n",
      "Epoch 2/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 979us/step - loss: 0.9203 - val_loss: 0.7233\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.7055 - val_loss: 0.6263\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6533 - val_loss: 0.6047\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.6030 - val_loss: 0.5883\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5907 - val_loss: 0.5743\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 0.5611\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5674 - val_loss: 0.5716\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.5531 - val_loss: 0.5513\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.5405 - val_loss: 0.5425\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.5550 - val_loss: 0.5409\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5390 - val_loss: 0.5442\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5450 - val_loss: 0.5343\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5341 - val_loss: 0.5633\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5298 - val_loss: 0.5312\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 0.5236\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5328 - val_loss: 0.5431\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5380 - val_loss: 0.5277\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5351 - val_loss: 0.5285\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5120 - val_loss: 0.5258\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5382\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.4928 - val_loss: 0.5341\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.5355\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5122 - val_loss: 0.5225\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.5294 - val_loss: 0.5256\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5413 - val_loss: 0.5267\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.5278\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.5326\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5251\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5318 - val_loss: 0.5339\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5181 - val_loss: 0.5187\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.5356 - val_loss: 0.5616\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5250\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5566\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5212 - val_loss: 0.5418\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 0.5579\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5444 - val_loss: 0.5241\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.5537\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5256\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5313 - val_loss: 0.5383\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5403 - val_loss: 0.5227\n",
      "121/121 [==============================] - 0s 592us/step - loss: 0.5565\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.9157 - val_loss: 5.8566\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8007 - val_loss: 5.7640\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6261 - val_loss: 5.9424\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 6.1266\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5984 - val_loss: 6.2831\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5779 - val_loss: 6.4476\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5413 - val_loss: 6.6001\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5465 - val_loss: 6.7292\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 6.8225\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5404 - val_loss: 6.9372\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5584 - val_loss: 7.0593\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5150 - val_loss: 7.1424\n",
      "121/121 [==============================] - 0s 579us/step - loss: 2.2585\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0396 - val_loss: 3.8812\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8889 - val_loss: 1.8299\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8208 - val_loss: 1.1938\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6712 - val_loss: 0.9044\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6599 - val_loss: 0.7499\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6478 - val_loss: 0.6499\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6181 - val_loss: 0.6101\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5984 - val_loss: 0.6431\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.6327\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5805 - val_loss: 0.6481\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.6042\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5673 - val_loss: 0.5588\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5429 - val_loss: 0.6051\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5586 - val_loss: 0.5976\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5894\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5400 - val_loss: 0.5534\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5280 - val_loss: 0.5863\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5623 - val_loss: 0.6101\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5433 - val_loss: 0.5481\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5466 - val_loss: 0.5805\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5889\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5640 - val_loss: 0.5828\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5360 - val_loss: 0.5514\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5458 - val_loss: 0.6093\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5132 - val_loss: 0.5599\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5361 - val_loss: 0.5760\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5356 - val_loss: 0.5792\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.5425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5265 - val_loss: 0.5534\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5370 - val_loss: 0.5364\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.5358\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4998 - val_loss: 0.5715\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5508 - val_loss: 0.5590\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5294 - val_loss: 0.5469\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.5504\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.5604\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5487 - val_loss: 0.5513\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5296 - val_loss: 0.5698\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5448 - val_loss: 0.5627\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5522 - val_loss: 0.5677\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4925 - val_loss: 0.5560\n",
      "121/121 [==============================] - 0s 608us/step - loss: 0.5424\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.9754 - val_loss: 0.8841\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7391 - val_loss: 0.5740\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5721 - val_loss: 0.5318\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5253 - val_loss: 0.5435\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.5326\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.5555\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5177 - val_loss: 0.5261\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5195 - val_loss: 0.5308\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.5335\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5266\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5257\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5185 - val_loss: 0.5277\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 0.5564\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5203 - val_loss: 0.5372\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5088 - val_loss: 0.5281\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5380 - val_loss: 0.5317\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5151 - val_loss: 0.5300\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5147 - val_loss: 0.5343\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5183 - val_loss: 0.5221\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5204\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5046 - val_loss: 0.5361\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5233\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5193 - val_loss: 0.5269\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.5301\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5208 - val_loss: 0.5365\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5218 - val_loss: 0.5277\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.5333\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 0.5300\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5224 - val_loss: 0.5188\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5395 - val_loss: 0.5329\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5256\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5044 - val_loss: 0.5393\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5477 - val_loss: 0.5254\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 0.5346\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5134 - val_loss: 0.5279\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5333 - val_loss: 0.5560\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.5383\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 0.5534\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5207 - val_loss: 0.5247\n",
      "121/121 [==============================] - 0s 615us/step - loss: 0.5527\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.7915 - val_loss: 0.9173\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8614 - val_loss: 0.6235\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6199 - val_loss: 0.6504\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6209 - val_loss: 0.7438\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5920 - val_loss: 0.8785\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5756 - val_loss: 1.0433\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5743 - val_loss: 1.2311\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5606 - val_loss: 1.4336\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5719 - val_loss: 1.6482\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5336 - val_loss: 1.8746\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5457 - val_loss: 2.1108\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5538 - val_loss: 2.3419\n",
      "121/121 [==============================] - 0s 725us/step - loss: 0.9921\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.7677 - val_loss: 11.0267\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.2683 - val_loss: 2.8017\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7582 - val_loss: 0.6724\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.5965\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6215 - val_loss: 0.5617\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5647 - val_loss: 0.5310\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5365 - val_loss: 0.5107\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5050 - val_loss: 0.4899\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4904 - val_loss: 0.4754\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4910 - val_loss: 0.4633\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4781 - val_loss: 0.4539\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.4449\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4558 - val_loss: 0.4427\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4354\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4296 - val_loss: 0.4330\n",
      "Epoch 16/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4297 - val_loss: 0.4288\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.434 - 0s 2ms/step - loss: 0.4346 - val_loss: 0.4205\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4488 - val_loss: 0.4223\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4226 - val_loss: 0.4126\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4095 - val_loss: 0.4148\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.4084\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4077 - val_loss: 0.4091\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4143 - val_loss: 0.4026\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 0.4015\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.4052\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4029 - val_loss: 0.3985\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4003 - val_loss: 0.3992\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.3927\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.3965\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.3917\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3957 - val_loss: 0.3978\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4125 - val_loss: 0.3966\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.3957\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.3873\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.3872\n",
      "Epoch 36/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.3862\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3894\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.3867\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.3840\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3874\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.3818\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.3843\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3976 - val_loss: 0.3823\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.3834\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3778 - val_loss: 0.3842\n",
      "Epoch 46/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3887 - val_loss: 0.3829\n",
      "Epoch 47/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.3796\n",
      "Epoch 48/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3806\n",
      "Epoch 49/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3665 - val_loss: 0.3802\n",
      "Epoch 50/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3869 - val_loss: 0.3775\n",
      "Epoch 51/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.3806\n",
      "Epoch 52/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3766\n",
      "Epoch 53/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.3791\n",
      "Epoch 54/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.3753\n",
      "Epoch 55/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.3774\n",
      "Epoch 56/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.3771\n",
      "Epoch 57/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3760\n",
      "Epoch 58/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3735\n",
      "Epoch 59/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3780 - val_loss: 0.3741\n",
      "Epoch 60/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.3743\n",
      "121/121 [==============================] - 0s 683us/step - loss: 0.3751\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.2926 - val_loss: 0.7472\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6923 - val_loss: 0.6533\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6107 - val_loss: 0.5574\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5801 - val_loss: 0.5251\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5208 - val_loss: 0.4959\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5002 - val_loss: 0.7549\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5135 - val_loss: 0.7985\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5832 - val_loss: 0.9543\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5516 - val_loss: 0.9320\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5855 - val_loss: 0.8573\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.7238\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5449 - val_loss: 0.6072\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4609 - val_loss: 0.4898\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4320\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.4250\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4263\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4162 - val_loss: 0.4112\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3840 - val_loss: 0.4101\n",
      "Epoch 19/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.4085\n",
      "Epoch 20/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.4069\n",
      "Epoch 21/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.4017\n",
      "Epoch 22/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3846 - val_loss: 0.4012\n",
      "Epoch 23/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4110 - val_loss: 0.3975\n",
      "Epoch 24/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3927 - val_loss: 0.3955\n",
      "Epoch 25/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.3911\n",
      "Epoch 26/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3773 - val_loss: 0.3908\n",
      "Epoch 27/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.3898\n",
      "Epoch 28/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.3906\n",
      "Epoch 29/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3969 - val_loss: 0.3849\n",
      "Epoch 30/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.3843\n",
      "Epoch 31/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.3836\n",
      "Epoch 32/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.3828\n",
      "Epoch 33/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3844 - val_loss: 0.3803\n",
      "Epoch 34/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3778\n",
      "Epoch 35/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3786\n",
      "Epoch 36/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.3750\n",
      "Epoch 37/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3763\n",
      "Epoch 38/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.3759\n",
      "Epoch 39/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.3736\n",
      "Epoch 40/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.3747\n",
      "Epoch 41/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3692\n",
      "Epoch 42/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.3701\n",
      "Epoch 43/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.3737\n",
      "Epoch 44/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.3675\n",
      "Epoch 45/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3660\n",
      "Epoch 46/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3554 - val_loss: 0.3665\n",
      "Epoch 47/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.3667\n",
      "Epoch 48/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 0.3664\n",
      "Epoch 49/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.3672\n",
      "Epoch 50/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3629\n",
      "Epoch 51/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.3655\n",
      "Epoch 52/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3762 - val_loss: 0.3637\n",
      "Epoch 53/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3427 - val_loss: 0.3612\n",
      "Epoch 54/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3625\n",
      "Epoch 55/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3638\n",
      "Epoch 56/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3506 - val_loss: 0.3613\n",
      "Epoch 57/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3633\n",
      "Epoch 58/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3431 - val_loss: 0.3643\n",
      "Epoch 59/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3622\n",
      "Epoch 60/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3564 - val_loss: 0.3640\n",
      "121/121 [==============================] - 0s 667us/step - loss: 0.3893\n",
      "Epoch 1/60\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.3542 - val_loss: 2.0896\n",
      "Epoch 2/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8039 - val_loss: 1.6469\n",
      "Epoch 3/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7263 - val_loss: 1.2275\n",
      "Epoch 4/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6265 - val_loss: 0.9324\n",
      "Epoch 5/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5954 - val_loss: 0.7346\n",
      "Epoch 6/60\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5585 - val_loss: 0.6101\n",
      "Epoch 7/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 0.5159\n",
      "Epoch 8/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5170 - val_loss: 0.5103\n",
      "Epoch 9/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5199 - val_loss: 0.5373\n",
      "Epoch 10/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5333 - val_loss: 0.5801\n",
      "Epoch 11/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4815 - val_loss: 0.6163\n",
      "Epoch 12/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4831 - val_loss: 0.6697\n",
      "Epoch 13/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4805 - val_loss: 0.7118\n",
      "Epoch 14/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 0.7890\n",
      "Epoch 15/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4610 - val_loss: 0.8315\n",
      "Epoch 16/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4537 - val_loss: 0.9040\n",
      "Epoch 17/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4511 - val_loss: 0.9451\n",
      "Epoch 18/60\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4249 - val_loss: 0.9933\n",
      "121/121 [==============================] - 0s 669us/step - loss: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\marksan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [-4.63341663e-01 -3.66621733e-01             nan             nan\n",
      "             nan -3.37986281e-01 -4.42408895e+26 -1.12017423e+00\n",
      " -6.95750992e-01 -4.48101342e-01]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x0000013FA5523F10>, as the constructor either does not set or modifies parameter n_neurons",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-8ef8b1940283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m rnd_search_cv.fit(X_train,y_train, epochs=60,\n\u001b[0m\u001b[0;32m     12\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[1;32mc:\\users\\marksan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marksan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    877\u001b[0m                 **self.best_params_))\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marksan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marksan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     86\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x0000013FA5523F10>, as the constructor either does not set or modifies parameter n_neurons"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0,1,2,3],\n",
    "    \"n_neurons\":np.arange(1,100),\n",
    "    \"learning_rate\":reciprocal(3e-4,3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train,y_train, epochs=60,\n",
    "                 validation_data=(X_val,y_val),\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
